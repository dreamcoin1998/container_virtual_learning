# Kubernetes中的网络策略：性能评估和安全分析

**作者**：Gerald Budigiri; Christoph Baumann; Jan Tobias Mühlberg; Eddy Truyen; Wouter Joosen
**原文**：https://ieeexplore.ieee.org/document/9482526

[TOC]

## 摘要

具有超高可靠性要求和低延迟要求的5G应用需要在移动网络中采用边缘计算解决方案。根据终端用户和第三方公司的需求，像k8s这样的容器编排框架已经进一步成为动态部署边缘应用的首选标准。不幸的是，复杂的网络和安全问题被强调为阻碍行业成功采用容器技术的挑战。安全挑战因（错误）概念而加剧，即安全的荣期间通信以牺牲性能为代价，但是这样中需求对于5G边缘计算用例来说都至关重要。为了追求低开销的安全方案，本论文研究网络策略，即k8s用于控制租户之间网络隔离的概念。我们评估Calico和Cilium基于ebpf的解决方案的性能开销，分析网络策略的安全性，突出网络策略的安全威胁，并概述相应的先进解决方案。我们的评估表明，网络策略时适合低延迟容器间通信的低开销安全解决方案。

## 前言

5G的出现为几个新的超可靠低延迟通信（URLLC）应用和用例打开了大门，如虚拟/增强现实（VR / AR），车辆到一切（V2X）和远程手术（RS），有望显着增加对计算和通信资源的需求。即使硬件功能最近有所进步，这些应用的严格性能要求仍然无法与实际的设备和网络功能相匹配[1]。为了解决这种不匹配问题，5G提供商采用了边缘计算（边缘计算是指在用户或数据源的物理位置或附近进行的计算，这样可以降低延迟，节省带宽。）和网络功能虚拟化（NFV）等几种技术概念。借助NFV，边缘计算平台虚拟化了网络功能模块，并将云计算能力扩展到终端用户附近的边缘设备，从而在多租户边缘生态系统中提供灵活性和敏捷性。
云原生计算趋势满足了这一发展，其中应用程序由使用无状态API相互交互的微服务组成。Kubernetes等灵活的编排框架之上使用轻量级和可以指的容器，而不是更资源密集型和启动缓慢基于VM的解决方案，使得云原生网络功能（CNF）成为5G边缘计算URLLC应用的更好选择。现在微服务架构正在被研究为边缘NFV的可能解决方案。
然而，复杂的网络和安全问题被强调为工业中容器采用的挑战。在多租户边缘计算云环境中，潜在的安全漏洞具有更高的严重性，在这些环境中，需要隔离属于不同房的微服务，以便他们只能在必要时进行交互。此外，对于RS和V2X等任务关键性5G应用，通信性能和安全性都不应受到影响。因此，必须为容器间通信找到低开销的安全解决方案。虽然行业和研究界已经做出了许多努力，来提高容器安全性，但其中大多数工作都集中在容器镜像，运行时，内核操作系统和k8s配置上，很少或根本没有考虑低延迟应用程序中的网络安全问题。k8s提供允许限制容器间通信的网络策略。虽然缺乏入侵检测等现代防火墙的高级功能，但不同网络策略仍然提供了合理的网络安全水平。在多租户环境中，它们通过将流量限制为仅允许互相通信的微服务，在租户之间提供可配置的网络隔离。虽然通过k8s网络策略API进行定义，但是策略的实施由自定义的容器网络接口（CNI）插件处理。之前的工作比较了不同CNI插件，并讨论了他们在多租户中的使用。相比之下，本文首次专门研究了k8s网络策略的性能开销和安全影响，并探讨了网络策略在边缘保护URLLC应用程序的适用性。我们做出以下贡献：

- 性能：我们评估了合适的CNI插件并且表明CNI插件的网络策略产生的性能开销可以忽略不计，该开销仅随策略数量和不同策略配置方式而略有不同。
- 安全：我们定义一个针对网络策略的攻击模型并且分析相应的威胁和漏洞。此外我们提出了合适的最先进的低延迟解决方案，以应对这些威胁。

在介绍这些结果之前，我们概述了k8s的网络策略的技术基础，功能和挑战。
本节介绍k8s、k8s中的网络策略，并且为什么它们在多租户边缘平台中的重要性。
此外，我们就CNI插件和网络策略支持进行比较，并解释Calico中的策略实施。

### A.Kubernetes(k8s)

k8s是容器编排的事实标准，它支持部署、拓展和管理容器化微服务应用程序并提供网络概念和对大规模互联微服务的支持。单个微服务应用程序在容器中运行，一个或多个紧密耦合的容器在Pod中运行，而pod在节点上运行，通过k8s主节点上运行的API服务进行控制。由于Pod是临时的，在拓展过程中动态启动或终止，因此服务对象用于为pod提供稳定的端点。此类和其他配置对象与控制k8s的编排过程的状态信息一起存储在分布式etcd数据库中。

### B.kubernetes 网络策略

在多租户平台，保护边缘用户的隐私是重要的。尽管k8s提供集群范围的命名空间用来提供隔离以进行管理和资源配额管理，单词支持不足以避免网络遍历。然而缺乏足够的网络分段是是一个一个经常被引用的高风险漏洞，该漏洞已被Equifax数据泄露等大型网络犯罪所利用。
网络策略通过显式声明允许和拒绝的连接，帮助提供限制pod之间（在和/或跨命名空间）以及pod与外部网络之间的流量所需的护栏。网络策略规范由podSelector组成，用于指定将受策略约束的pod，以及用于指定策略类型（入口或出口）的策略类型。入口规则指定允许入站的流量到目标Pod，出口规则指定允许的来自目标pod的出站流量。每个规则都由一个NetworkPolicyPeer组成，用于通过无类域间路由（CIDR）表示法选择连接另一端允许流量的pod，该表示法指定IP地址块，命名空间或pod标签选择一个NetworkPolicyPort，它允许显示指定可能与Pod通信的端口或网络协议。网络策略可能是累加的，如果多个策略选择一个Pod，则流量将限制在这些策略的入口/出口规则并集所允许的范围内。

### CNI插件和网络模式

由于没有内置功能来实施网络策略，k8s依靠CNI插件来实施。CNI插件作为附加组件安装，该附加组件可以作为容器运行，也可以依赖开源k8s组件。虽然存在许多CNI插件，但只有Calico和Cilium支持使用ebpf实施网络策略，ebpf是iptables的高性能替代方案。事实上，再将ebpf与iptables进行比较时，我们观察到延迟减轻了0.7到0.8倍，节点间和节点内场景的吞吐量分别提高了3.5倍和1.2倍。测量是在封闭实验室的OpenStack测试平台上使用Calico进行的（参见III-A和III-B部分，了解测试平台的实验设置和规格）。Calico和Cilium都使用流量控制钩子，在Pod的虚拟以太网接口的入口和出口处附加ebpf程序。
根据主机模式评估Calico ebpf和Cilium ebpf性能，在该模式下，基准测试在OpenStack VM实例上运行。结果表明，尽管ebpf数据平面有优势，但是CNI插件（可能还有整个K8s架构）的开销仍然很大，特别是对于节点间的通信。结果进一步表明，Calico ebpf在节点间场景中的表现明显优于Cilium ebpf。这是因为Cilium默认以隧道模式运行，这会引发封装标头，而Calico在每个节点上使用Linux内核路由支持来提供纯第三层网络解决方案，从而降低开销。因此，Calico ebpf被用于本文其余性能评估。Calico和主机节点内测量在裸机上重复，其中主机结果在延迟和吞吐量方面分别高出8.7%和10.7%，而OpenStack则为33%和31%。OpenStack上较高的开销表示VM的基于OpenStack的网络配置与Pod基于CNI的网络配置之间可能存在负面干扰。
